# HUM v1 configuration

# Basic
seed: 42
dataset: m_IOATBC
item_count: 0
test_only: false
user_k: 5
item_k: 5
ratio: 1.0
mask_ratio: 0.0
nega_count: 1000
train_nega_count: 10
wandb_run_name: hum-v1-user-item
prompt_selection: 0
flag_generative_loss: false
flag_kl_loss: false
flag_tail_loss: false
flag_special_test:
similarity_temperature: 1
version: hum_v1  # Using the new hum_v1 version

# Checkpoint  
output: None
pretrained_path: None
load:
root_path: Qwen/
data_path: dataset
save_by_step: 5000

# Hardware
multiGPU: true
distributed: true
num_workers: 1
num_gpus: 4
num_nodes: 1
local_rank: -1
port: 12344

# Model
lora: true
valid_first: false
backbone: Qwen2.5-1.5B
max_length: 512
max_token_length: 1024
item_emb_dim: 128
hidden_size: 1536
hidden_dropout_prob: 0.1
hidden_act: relu

# LoRA 
lora_r: 8
lora_alpha: 16
lora_dropout: 0.1
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
pretrain_lora: true

# Training
batch_size: 2  # Consistent with the current run configuration
alpha: 1
beta_kl: 1
early_stop_step_num: 5
valid_batch_size: null
optim: adamw
warmup_ratio: 0.001
weight_decay: 0.001
clip_grad_norm: 1.0
gradient_accumulation_steps: 1
lr: 5e-5
adam_eps: 1e-4
adam_beta1: 0.9
adam_beta2: 0.999
epoch: 50
dropout: 0.1
skip_valid: false
use_cache: false
start_epoch: 0

# Others
valid_ratio: 0.1
